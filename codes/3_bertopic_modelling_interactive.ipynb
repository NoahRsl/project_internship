{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import polars as pl\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cols = [\"title\",\n",
    "        \"year\",\n",
    "        \"primary_topic\",\n",
    "        \"abstract\",\n",
    "        \"cited_by_count\",\n",
    "        \"mncs\",\n",
    "        \"countries_distinct_count\",\n",
    "        \"institutions_distinct_count\",\n",
    "        \"referenced_works_count\",\n",
    "        \"authors_count\",\n",
    "        \"review\",\n",
    "        \"meta_analysis\",\n",
    "        \"mean_past_contributions_authors\",\n",
    "        \"mean_past_mncs_authors\",\n",
    "        \"mean_past_contributions_institutions\",\n",
    "        \"mean_past_mncs_institutions\"]\n",
    "cols += [f\"cited_by_count_{i}\" for i in range(2012, 2025)]\n",
    "\n",
    "# Ran with Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "data = pl.read_parquet(\"/content/drive/MyDrive/works_pre_topics.parquet\", columns = cols) #\n",
    "\n",
    "data = data.with_columns(\n",
    "    title_abstract = pl.col(\"title\") + \". \" + pl.col(\"abstract\")\n",
    ")\n",
    "\n",
    "data = data.filter(pl.col(\"title_abstract\").is_not_null())\n",
    "data = data.filter(pl.col(\"title_abstract\") != \"\")\n",
    "\n",
    "custom_stop_words = [\n",
    "    \"current\", \"board\", \"editorial\", \"thank\", \"contents\", \"table\", \"availability\",\n",
    "    \"matter\", \"talking\", \"question\", \"older\", \"erratum\", \"eradication\", \"approval\",\n",
    "    \"global\", \"correction\", \"written\", \"issue\", \"information\", \"publication\",\n",
    "    \"publishing\", \"implementing\", \"noticeboard\", \"problems\"\n",
    "]\n",
    "\n",
    "all_stop_words = set(ENGLISH_STOP_WORDS).union(custom_stop_words)\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = wordpunct_tokenize(text.lower())\n",
    "    return ' '.join([word for word in tokens if word not in all_stop_words])\n",
    "\n",
    "cleaned_title_abstract = [clean_text(doc) for doc in tqdm(data[\"title_abstract\"], desc=\"Cleaning\")]\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")\n",
    "\n",
    "batch_size = 128\n",
    "n_docs = len(cleaned_title_abstract)\n",
    "n_batches = (n_docs // batch_size) + 1\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "for i in tqdm(range(n_batches), desc=\"Text Encoding...\", ncols=100):\n",
    "    start = i * batch_size\n",
    "    end = start + batch_size\n",
    "    batch_texts = cleaned_title_abstract[start:end]\n",
    "\n",
    "    batch_embeddings = embedding_model.encode(\n",
    "        batch_texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=torch._functionalize_set_storage_changed\n",
    "    )\n",
    "\n",
    "    embeddings_list.append(batch_embeddings)\n",
    "\n",
    "embeddings = np.vstack(embeddings_list)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(embeddings)\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    low_memory=True\n",
    ")\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=list(all_stop_words),\n",
    "    lowercase=True,\n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\"\n",
    ")\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=30,\n",
    "    min_samples=5,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=None,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    language=\"english\",\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if isinstance(embeddings, torch.Tensor):\n",
    "    embeddings_np = embeddings.cpu().numpy()\n",
    "else:\n",
    "    embeddings_np = embeddings\n",
    "\n",
    "for _ in tqdm(range(3), desc=\"Clusturing Init...\", ncols=100):\n",
    "    gc.collect()\n",
    "\n",
    "sample_size = min(100000, len(cleaned_title_abstract))\n",
    "sample_texts = cleaned_title_abstract[:sample_size]\n",
    "sample_embeddings = embeddings_np[:sample_size]\n",
    "\n",
    "topic_model.fit(sample_texts, embeddings=sample_embeddings)\n",
    "\n",
    "batch_size = 1000\n",
    "n_samples = len(cleaned_title_abstract)\n",
    "\n",
    "all_topics = []\n",
    "all_probs = []\n",
    "\n",
    "for start_idx in tqdm(range(0, n_samples, batch_size), desc=\"Processing batches\"):\n",
    "    end_idx = min(start_idx + batch_size, n_samples)\n",
    "    batch_texts = cleaned_title_abstract[start_idx:end_idx]\n",
    "    batch_embeddings = embeddings_np[start_idx:end_idx]\n",
    "\n",
    "    topics, probs = topic_model.transform(batch_texts, embeddings=batch_embeddings)\n",
    "\n",
    "    all_topics.extend(topics)\n",
    "    all_probs.append(probs)\n",
    "\n",
    "    del batch_texts, batch_embeddings, topics, probs\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "topic_model.save(\"/content/drive/MyDrive/bertopic_model_pharmacology2\")\n",
    "all_topics_series = pl.Series(\"topic\", all_topics, dtype=pl.Int64)\n",
    "data = data.with_columns(all_topics_series)\n",
    "data.write_parquet(\"/content/drive/MyDrive/works_post_topics2.parquet\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "topic_modelling = BERTopic.load(\"/content/drive/MyDrive/bertopic_model_pharmacology2\", embedding_model=embedding_model)\n",
    "test = topic_modelling.get_topics()\n",
    "test = pd.DataFrame(test)\n",
    "test.to_csv(\"topic_words2.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74471b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "topic_freq_df = topic_modelling.get_topic_freq()\n",
    "topic_freq_df = topic_freq_df[~topic_freq_df['Topic'].isin([-1, 0])]\n",
    "\n",
    "\n",
    "top_n = 20\n",
    "top_topics = topic_freq_df.head(top_n)['Topic'].tolist()\n",
    "\n",
    "\n",
    "# Load topic words from CSV but added, \n",
    "# Not sure if it works properly, need to check \n",
    "\n",
    "topics_csv = pd.read_csv(\"topic_words2.csv\", index_col=0) \n",
    "\n",
    "def get_topic_words(x):\n",
    "    try:\n",
    "        word1 = topics_csv.loc[0, str(x)].split(\",\")[0].replace(\"('\",\"\").replace(\"'\",\"\").strip()\n",
    "        word2 = topics_csv.loc[1, str(x)].split(\",\")[0].replace(\"('\",\"\").replace(\"'\",\"\").strip()\n",
    "        return f\"{word1}_{word2}\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "topic_names = {tid: get_topic_words(tid) for tid in top_topics}\n",
    "\n",
    "embeddings = topic_modelling.topic_embeddings_\n",
    "indices = [topic_modelling.get_topic_freq().index[topic_modelling.get_topic_freq()['Topic']==tid][0] for tid in top_topics]\n",
    "emb_top = embeddings[indices]\n",
    "\n",
    "sim = cosine_similarity(emb_top)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "im = plt.imshow(sim, cmap='viridis')\n",
    "plt.colorbar(im, label='Cosine Similarity')\n",
    "\n",
    "names = [topic_names[t] for t in top_topics]\n",
    "plt.xticks(np.arange(top_n), names, rotation=90, fontsize=10)\n",
    "plt.yticks(np.arange(top_n), names, fontsize=10)\n",
    "plt.title(\"Topic Similarity Heatmap (Top 20 Topics)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/content/drive/MyDrive/pngs/topic_similarity_heatmap_top20.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "umap_embeddings = topic_modelling.topic_embeddings_[indices]\n",
    "freqs = topic_freq_df.set_index('Topic')['Count'].to_dict()\n",
    "\n",
    "x = umap_embeddings[:,0]\n",
    "y = umap_embeddings[:,1]\n",
    "sizes = [freqs.get(t, 10) for t in top_topics]\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(x, y, s=np.sqrt(sizes)*10, alpha=0.7, c='steelblue', edgecolor='k')\n",
    "for i, tid in enumerate(top_topics):\n",
    "    plt.text(x[i], y[i], topic_names[tid], fontsize=10, ha='center', va='center')\n",
    "\n",
    "plt.title(\"Topic Map (Top 20 Topics)\")\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/content/drive/MyDrive/pngs/topic_map_top20.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "topic_id = top_topics[0]\n",
    "topic_name = get_topic_words(topic_id)\n",
    "\n",
    "words_scores = topic_modelling.get_topic(topic_id)\n",
    "top_words = words_scores[:10]\n",
    "labels = [w[0] for w in top_words]\n",
    "scores = [w[1] for w in top_words]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(labels, scores, color='steelblue')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"Top 10 Words for Topic: {topic_name}\")\n",
    "plt.xlabel(\"c-TF-IDF Score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"/content/drive/MyDrive/pngs/top10_words_{topic_name}.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
